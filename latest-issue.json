[{"body":"\n## Description of the Error\n\nA common issue when working with Next.js API routes involves sending large responses that exceed the Node.js default response buffer size. This often results in the dreaded `ERR_HTTP_HEADERS_SENT` error.  This error occurs because you're attempting to send headers (like status codes and content-type) after the response stream has already started, usually because of a large response body being written before headers are sent.  This prevents the server from sending a properly formatted HTTP response.\n\n\n## Step-by-Step Code Fix\n\nLet's assume you have an API route that fetches and returns a large JSON dataset:\n\n**Problem Code (pages/api/data.js):**\n\n```javascript\n// pages/api/data.js\nexport default async function handler(req, res) {\n  const largeDataset = await fetch('https://example.com/huge-dataset.json').then(r => r.json()); // Replace with your data source\n\n  res.status(200).json(largeDataset);\n}\n```\n\nThis code might fail with `ERR_HTTP_HEADERS_SENT` if `largeDataset` is extremely large.  Here's how to fix it:\n\n\n**Solution Code (pages/api/data.js):**\n\n```javascript\n// pages/api/data.js\nimport { pipeline } from 'node:stream/promises';\nimport { Readable } from 'node:stream';\n\nexport default async function handler(req, res) {\n  res.setHeader('Content-Type', 'application/json'); // Set headers early\n\n  const largeDataset = await fetch('https://example.com/huge-dataset.json').then(r => r.json());\n\n  const readableStream = new Readable({\n    read() {\n      this.push(JSON.stringify(largeDataset)); // Convert to String first for optimal streaming\n      this.push(null); // Signal end of stream\n    }\n  });\n\n\n  try {\n    await pipeline(readableStream, res);\n  } catch (err) {\n    console.error('Pipeline failed.', err);\n    res.status(500).json({ error: 'Failed to process request' });\n  }\n}\n```\n\n**Explanation:**\n\n1. **`res.setHeader('Content-Type', 'application/json');`:** We set the headers *before* sending any data.  This is crucial for avoiding `ERR_HTTP_HEADERS_SENT`.\n\n2. **`Readable` Stream:** We create a `Readable` stream using the `JSON.stringify` output of the `largeDataset`. This allows us to stream the data instead of buffering it all in memory at once.\n\n3. **`pipeline`:** The `pipeline` utility from `node:stream/promises` handles the streaming efficiently. It pipes the data from the `Readable` stream to the response (`res`).  The `try...catch` block handles potential errors during the streaming process.\n\n4. **`JSON.stringify`:**  Convert the JSON data to a string *before* pushing it into the stream for better performance, as the `Readable` expects raw data.\n\n\n## External References\n\n* **Node.js `stream` documentation:** [https://nodejs.org/api/stream.html](https://nodejs.org/api/stream.html)\n* **Next.js API Routes documentation:** [https://nextjs.org/docs/api-routes/introduction](https://nextjs.org/docs/api-routes/introduction)\n* **Understanding `pipeline`:** [https://nodejs.org/api/stream.html#stream_class_stream_pipeline](https://nodejs.org/api/stream.html#stream_class_stream_pipeline)\n\n\n## Explanation of the Solution\n\nThe core issue is memory management.  Large JSON responses require significant memory, leading to the buffer overflow.  By streaming the response using the `Readable` stream and `pipeline`, we avoid loading the entire JSON into memory at once.  Instead, data is processed and sent in smaller chunks, solving the memory problem and preventing the `ERR_HTTP_HEADERS_SENT` error.\n\n\nCopyrights (c) OpenRockets Open-source Network. Free to use, copy, share, edit or publish.\n","number":1196,"title":"Next.js API Routes: Handling Large Responses and Avoiding `ERR_HTTP_HEADERS_SENT`"}]
