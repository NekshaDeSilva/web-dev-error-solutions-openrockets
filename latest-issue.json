[{"body":"\nThis document addresses a common issue developers encounter when working with Next.js API routes: exceeding the request timeout limit when returning large responses.  This often manifests as a 504 Gateway Timeout error on the client-side.\n\n\n## Description of the Error\n\nNext.js API routes, by default, have a timeout limit.  If your API route takes too long to generate and send a response (e.g., processing a large dataset, complex calculations, or interacting with a slow external service), the request will timeout before the complete response is sent to the client. This results in a frustrating 504 error for the user and a failed API call.\n\n\n## Step-by-Step Code Fix\n\nLet's assume our API route needs to generate a large JSON response containing a significant amount of data.  Here's how to handle this effectively:\n\n**Problem Code (Illustrative):**\n\n```javascript\n// pages/api/largedata.js\nexport default async function handler(req, res) {\n  const largeDataset = generateLargeDataset(); // Takes a long time\n  res.status(200).json(largeDataset);\n}\n\nfunction generateLargeDataset() {\n  // Simulates generating a large dataset - replace with your actual logic\n  const data = [];\n  for (let i = 0; i < 100000; i++) {\n    data.push({ id: i, value: `Value ${i}` });\n  }\n  return data;\n}\n```\n\n**Solution Code (Streaming the Response):**\n\n```javascript\n// pages/api/largedata.js\nimport { pipeline } from 'stream/promises';\nimport { Readable } from 'stream';\n\nexport default async function handler(req, res) {\n  const largeDataset = generateLargeDataset();\n\n  const readableStream = new Readable({\n    objectMode: true,\n    read() {\n      for (const item of largeDataset) {\n        this.push(item);\n      }\n      this.push(null); // Signal end of stream\n    },\n  });\n\n  res.setHeader('Content-Type', 'application/json');\n  await pipeline(readableStream, res);\n}\n\nfunction generateLargeDataset() {\n  // Simulates generating a large dataset - replace with your actual logic\n  const data = [];\n  for (let i = 0; i < 100000; i++) {\n    data.push({ id: i, value: `Value ${i}` });\n  }\n  return data;\n}\n\n```\n\nThis solution uses Node.js's `stream/promises` to create a readable stream.  The `largeDataset` is pushed into the stream, and `pipeline` efficiently sends it to the response.  This avoids loading the entire dataset into memory at once, preventing timeout issues.  Crucially, this uses `objectMode: true` to handle JSON objects correctly.\n\n**Alternative Solution (Chunking):**\n\nIf you're not comfortable with streams, you can chunk your response:\n\n```javascript\n// pages/api/largedata.js\nexport default async function handler(req, res) {\n  const largeDataset = generateLargeDataset();\n  const chunkSize = 1000; // Adjust as needed\n\n  for (let i = 0; i < largeDataset.length; i += chunkSize) {\n    const chunk = largeDataset.slice(i, i + chunkSize);\n    res.write(JSON.stringify(chunk)); // Note: JSON.stringify for each chunk\n    await new Promise(resolve => setTimeout(resolve, 10)); //Optional small delay for less load\n  }\n  res.end();\n}\n\n// ... (generateLargeDataset function remains the same)\n```\n\n\n## Explanation\n\nThe core issue is the amount of time it takes to process and send the entire JSON response.  Streaming avoids this by sending the data incrementally. Each chunk of data gets sent as soon as it's available. The client starts receiving the data almost immediately.  Chunking achieves a similar result by dividing the data into smaller parts, preventing memory overload and timeout errors.\n\n\n## External References\n\n* [Node.js Streams Documentation](https://nodejs.org/api/stream.html)\n* [Next.js API Routes](https://nextjs.org/docs/api-routes/introduction)\n* [Handling Large Files in Node.js](https://dev.to/satwikkansal/how-to-handle-large-files-in-node-js-3g1i)\n\n\n## Copyright (c) OpenRockets Open-source Network. Free to use, copy, share, edit or publish.\n","number":915,"title":"Next.js API Routes: Handling Large Responses and Avoiding Timeouts"}]
